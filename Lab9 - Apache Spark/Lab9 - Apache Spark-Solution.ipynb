{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1G6hktRBRz7J"
      },
      "outputs": [],
      "source": [
        "def get_path(dataset_name,env_name='colab'):\n",
        "    \"\"\"\n",
        "    This function is used to return the path of the dataset you want to use. \n",
        "    \n",
        "    @params:\n",
        "    dataset_name: the name of the dataset. \n",
        "    env_name: it has two values either local, or colab the default is colab\n",
        "    \"\"\"\n",
        "    prefix = 'https://raw.githubusercontent.com/mohamed-ashry7/Data-Engineering-Lab/main/Datasets/'\n",
        "    if env_name == 'colab':\n",
        "        return prefix+dataset_name\n",
        "    else:\n",
        "        return f'../Datasets/{dataset_name}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A9dWl21UnX3",
        "outputId": "17fb660d-a322-4107-fd80-b7c789c68455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,489 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,929 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [716 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [749 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Fetched 13.7 MB in 4s (3,428 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "59 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sXAjD3maUqPQ"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-VpRr78W5-ix"
      },
      "outputs": [],
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JxTq4BzAUxVg"
      },
      "outputs": [],
      "source": [
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yC62VdjK7PX8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1snDBPja_U5r"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VKkn476PBV1T"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ82nj8i_AqL",
        "outputId": "8bc853bf-3d9e-40ef-e51b-783c8fd788fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 47.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=819775c61f9f9c5135ec000e03291866931be44dfef316a8beba2e82fd54a0f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNq1Q-6oouss"
      },
      "source": [
        "To work on data using Apache Spark, we first need to instantiate a spark session, this session is going to interact with the Spark Context to manage the data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5t2y6yQI70FN"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.appName(\"Challenge 1\").getOrCreate()  # use all cores of your CPU - each core has thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng_RrVwuvVg1"
      },
      "source": [
        "#### **Challenge 1 - SQL and PySpark Querying**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1hyDLUeCue8"
      },
      "source": [
        "Write a PySpark code to read all the files in the `by-day` folder, and displays a table that shows the ID of the customer and the the total amount they paid for all their purchases. Make sure to drop any stock that has the word *WATER* in it before calculating the total price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "raMFEKUo5HZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c248304-21e5-476b-d99f-c6fc1a995db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-07 20:15:53--  https://raw.githubusercontent.com/mohamed-ashry7/Data-Engineering-Lab/main/Datasets/by-day.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8194140 (7.8M) [application/zip]\n",
            "Saving to: ‘by-day.zip’\n",
            "\n",
            "by-day.zip          100%[===================>]   7.81M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-01-07 20:15:54 (93.3 MB/s) - ‘by-day.zip’ saved [8194140/8194140]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Your answer here\n",
        "url = get_path(\"by-day.zip\")\n",
        "!wget $url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MRtfih1LVPp",
        "outputId": "7c29f3df-f677-4600-d451-c09c77106a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  by-day.zip\n",
            "  inflating: by-day/2010-12-01.csv   \n",
            "  inflating: by-day/2010-12-02.csv   \n",
            "  inflating: by-day/2010-12-03.csv   \n",
            "  inflating: by-day/2010-12-05.csv   \n",
            "  inflating: by-day/2010-12-06.csv   \n",
            "  inflating: by-day/2010-12-07.csv   \n",
            "  inflating: by-day/2010-12-08.csv   \n",
            "  inflating: by-day/2010-12-09.csv   \n",
            "  inflating: by-day/2010-12-10.csv   \n",
            "  inflating: by-day/2010-12-12.csv   \n",
            "  inflating: by-day/2010-12-13.csv   \n",
            "  inflating: by-day/2010-12-14.csv   \n",
            "  inflating: by-day/2010-12-15.csv   \n",
            "  inflating: by-day/2010-12-16.csv   \n",
            "  inflating: by-day/2010-12-17.csv   \n",
            "  inflating: by-day/2010-12-19.csv   \n",
            "  inflating: by-day/2010-12-20.csv   \n",
            "  inflating: by-day/2010-12-21.csv   \n",
            "  inflating: by-day/2010-12-22.csv   \n",
            "  inflating: by-day/2010-12-23.csv   \n",
            "  inflating: by-day/2011-01-04.csv   \n",
            "  inflating: by-day/2011-01-05.csv   \n",
            "  inflating: by-day/2011-01-06.csv   \n",
            "  inflating: by-day/2011-01-07.csv   \n",
            "  inflating: by-day/2011-01-09.csv   \n",
            "  inflating: by-day/2011-01-10.csv   \n",
            "  inflating: by-day/2011-01-11.csv   \n",
            "  inflating: by-day/2011-01-12.csv   \n",
            "  inflating: by-day/2011-01-13.csv   \n",
            "  inflating: by-day/2011-01-14.csv   \n",
            "  inflating: by-day/2011-01-16.csv   \n",
            "  inflating: by-day/2011-01-17.csv   \n",
            "  inflating: by-day/2011-01-18.csv   \n",
            "  inflating: by-day/2011-01-19.csv   \n",
            "  inflating: by-day/2011-01-20.csv   \n",
            "  inflating: by-day/2011-01-21.csv   \n",
            "  inflating: by-day/2011-01-23.csv   \n",
            "  inflating: by-day/2011-01-24.csv   \n",
            "  inflating: by-day/2011-01-25.csv   \n",
            "  inflating: by-day/2011-01-26.csv   \n",
            "  inflating: by-day/2011-01-27.csv   \n",
            "  inflating: by-day/2011-01-28.csv   \n",
            "  inflating: by-day/2011-01-30.csv   \n",
            "  inflating: by-day/2011-01-31.csv   \n",
            "  inflating: by-day/2011-02-01.csv   \n",
            "  inflating: by-day/2011-02-02.csv   \n",
            "  inflating: by-day/2011-02-03.csv   \n",
            "  inflating: by-day/2011-02-04.csv   \n",
            "  inflating: by-day/2011-02-06.csv   \n",
            "  inflating: by-day/2011-02-07.csv   \n",
            "  inflating: by-day/2011-02-08.csv   \n",
            "  inflating: by-day/2011-02-09.csv   \n",
            "  inflating: by-day/2011-02-10.csv   \n",
            "  inflating: by-day/2011-02-11.csv   \n",
            "  inflating: by-day/2011-02-13.csv   \n",
            "  inflating: by-day/2011-02-14.csv   \n",
            "  inflating: by-day/2011-02-15.csv   \n",
            "  inflating: by-day/2011-02-16.csv   \n",
            "  inflating: by-day/2011-02-17.csv   \n",
            "  inflating: by-day/2011-02-18.csv   \n",
            "  inflating: by-day/2011-02-20.csv   \n",
            "  inflating: by-day/2011-02-21.csv   \n",
            "  inflating: by-day/2011-02-22.csv   \n",
            "  inflating: by-day/2011-02-23.csv   \n",
            "  inflating: by-day/2011-02-24.csv   \n",
            "  inflating: by-day/2011-02-25.csv   \n",
            "  inflating: by-day/2011-02-27.csv   \n",
            "  inflating: by-day/2011-02-28.csv   \n",
            "  inflating: by-day/2011-03-01.csv   \n",
            "  inflating: by-day/2011-03-02.csv   \n",
            "  inflating: by-day/2011-03-03.csv   \n",
            "  inflating: by-day/2011-03-04.csv   \n",
            "  inflating: by-day/2011-03-06.csv   \n",
            "  inflating: by-day/2011-03-07.csv   \n",
            "  inflating: by-day/2011-03-08.csv   \n",
            "  inflating: by-day/2011-03-09.csv   \n",
            "  inflating: by-day/2011-03-10.csv   \n",
            "  inflating: by-day/2011-03-11.csv   \n",
            "  inflating: by-day/2011-03-13.csv   \n",
            "  inflating: by-day/2011-03-14.csv   \n",
            "  inflating: by-day/2011-03-15.csv   \n",
            "  inflating: by-day/2011-03-16.csv   \n",
            "  inflating: by-day/2011-03-17.csv   \n",
            "  inflating: by-day/2011-03-18.csv   \n",
            "  inflating: by-day/2011-03-20.csv   \n",
            "  inflating: by-day/2011-03-21.csv   \n",
            "  inflating: by-day/2011-03-22.csv   \n",
            "  inflating: by-day/2011-03-23.csv   \n",
            "  inflating: by-day/2011-03-24.csv   \n",
            "  inflating: by-day/2011-03-25.csv   \n",
            "  inflating: by-day/2011-03-27.csv   \n",
            "  inflating: by-day/2011-03-28.csv   \n",
            "  inflating: by-day/2011-03-29.csv   \n",
            "  inflating: by-day/2011-03-30.csv   \n",
            "  inflating: by-day/2011-03-31.csv   \n",
            "  inflating: by-day/2011-04-01.csv   \n",
            "  inflating: by-day/2011-04-03.csv   \n",
            "  inflating: by-day/2011-04-04.csv   \n",
            "  inflating: by-day/2011-04-05.csv   \n",
            "  inflating: by-day/2011-04-06.csv   \n",
            "  inflating: by-day/2011-04-07.csv   \n",
            "  inflating: by-day/2011-04-08.csv   \n",
            "  inflating: by-day/2011-04-10.csv   \n",
            "  inflating: by-day/2011-04-11.csv   \n",
            "  inflating: by-day/2011-04-12.csv   \n",
            "  inflating: by-day/2011-04-13.csv   \n",
            "  inflating: by-day/2011-04-14.csv   \n",
            "  inflating: by-day/2011-04-15.csv   \n",
            "  inflating: by-day/2011-04-17.csv   \n",
            "  inflating: by-day/2011-04-18.csv   \n",
            "  inflating: by-day/2011-04-19.csv   \n",
            "  inflating: by-day/2011-04-20.csv   \n",
            "  inflating: by-day/2011-04-21.csv   \n",
            "  inflating: by-day/2011-04-26.csv   \n",
            "  inflating: by-day/2011-04-27.csv   \n",
            "  inflating: by-day/2011-04-28.csv   \n",
            "  inflating: by-day/2011-05-01.csv   \n",
            "  inflating: by-day/2011-05-03.csv   \n",
            "  inflating: by-day/2011-05-04.csv   \n",
            "  inflating: by-day/2011-05-05.csv   \n",
            "  inflating: by-day/2011-05-06.csv   \n",
            "  inflating: by-day/2011-05-08.csv   \n",
            "  inflating: by-day/2011-05-09.csv   \n",
            "  inflating: by-day/2011-05-10.csv   \n",
            "  inflating: by-day/2011-05-11.csv   \n",
            "  inflating: by-day/2011-05-12.csv   \n",
            "  inflating: by-day/2011-05-13.csv   \n",
            "  inflating: by-day/2011-05-15.csv   \n",
            "  inflating: by-day/2011-05-16.csv   \n",
            "  inflating: by-day/2011-05-17.csv   \n",
            "  inflating: by-day/2011-05-18.csv   \n",
            "  inflating: by-day/2011-05-19.csv   \n",
            "  inflating: by-day/2011-05-20.csv   \n",
            "  inflating: by-day/2011-05-22.csv   \n",
            "  inflating: by-day/2011-05-23.csv   \n",
            "  inflating: by-day/2011-05-24.csv   \n",
            "  inflating: by-day/2011-05-25.csv   \n",
            "  inflating: by-day/2011-05-26.csv   \n",
            "  inflating: by-day/2011-05-27.csv   \n",
            "  inflating: by-day/2011-05-29.csv   \n",
            "  inflating: by-day/2011-05-31.csv   \n",
            "  inflating: by-day/2011-06-01.csv   \n",
            "  inflating: by-day/2011-06-02.csv   \n",
            "  inflating: by-day/2011-06-03.csv   \n",
            "  inflating: by-day/2011-06-05.csv   \n",
            "  inflating: by-day/2011-06-06.csv   \n",
            "  inflating: by-day/2011-06-07.csv   \n",
            "  inflating: by-day/2011-06-08.csv   \n",
            "  inflating: by-day/2011-06-09.csv   \n",
            "  inflating: by-day/2011-06-10.csv   \n",
            "  inflating: by-day/2011-06-12.csv   \n",
            "  inflating: by-day/2011-06-13.csv   \n",
            "  inflating: by-day/2011-06-14.csv   \n",
            "  inflating: by-day/2011-06-15.csv   \n",
            "  inflating: by-day/2011-06-16.csv   \n",
            "  inflating: by-day/2011-06-17.csv   \n",
            "  inflating: by-day/2011-06-19.csv   \n",
            "  inflating: by-day/2011-06-20.csv   \n",
            "  inflating: by-day/2011-06-21.csv   \n",
            "  inflating: by-day/2011-06-22.csv   \n",
            "  inflating: by-day/2011-06-23.csv   \n",
            "  inflating: by-day/2011-06-24.csv   \n",
            "  inflating: by-day/2011-06-26.csv   \n",
            "  inflating: by-day/2011-06-27.csv   \n",
            "  inflating: by-day/2011-06-28.csv   \n",
            "  inflating: by-day/2011-06-29.csv   \n",
            "  inflating: by-day/2011-06-30.csv   \n",
            "  inflating: by-day/2011-07-01.csv   \n",
            "  inflating: by-day/2011-07-03.csv   \n",
            "  inflating: by-day/2011-07-04.csv   \n",
            "  inflating: by-day/2011-07-05.csv   \n",
            "  inflating: by-day/2011-07-06.csv   \n",
            "  inflating: by-day/2011-07-07.csv   \n",
            "  inflating: by-day/2011-07-08.csv   \n",
            "  inflating: by-day/2011-07-10.csv   \n",
            "  inflating: by-day/2011-07-11.csv   \n",
            "  inflating: by-day/2011-07-12.csv   \n",
            "  inflating: by-day/2011-07-13.csv   \n",
            "  inflating: by-day/2011-07-14.csv   \n",
            "  inflating: by-day/2011-07-15.csv   \n",
            "  inflating: by-day/2011-07-17.csv   \n",
            "  inflating: by-day/2011-07-18.csv   \n",
            "  inflating: by-day/2011-07-19.csv   \n",
            "  inflating: by-day/2011-07-20.csv   \n",
            "  inflating: by-day/2011-07-21.csv   \n",
            "  inflating: by-day/2011-07-22.csv   \n",
            "  inflating: by-day/2011-07-24.csv   \n",
            "  inflating: by-day/2011-07-25.csv   \n",
            "  inflating: by-day/2011-07-26.csv   \n",
            "  inflating: by-day/2011-07-27.csv   \n",
            "  inflating: by-day/2011-07-28.csv   \n",
            "  inflating: by-day/2011-07-29.csv   \n",
            "  inflating: by-day/2011-07-31.csv   \n",
            "  inflating: by-day/2011-08-01.csv   \n",
            "  inflating: by-day/2011-08-02.csv   \n",
            "  inflating: by-day/2011-08-03.csv   \n",
            "  inflating: by-day/2011-08-04.csv   \n",
            "  inflating: by-day/2011-08-05.csv   \n",
            "  inflating: by-day/2011-08-07.csv   \n",
            "  inflating: by-day/2011-08-08.csv   \n",
            "  inflating: by-day/2011-08-09.csv   \n",
            "  inflating: by-day/2011-08-10.csv   \n",
            "  inflating: by-day/2011-08-11.csv   \n",
            "  inflating: by-day/2011-08-12.csv   \n",
            "  inflating: by-day/2011-08-14.csv   \n",
            "  inflating: by-day/2011-08-15.csv   \n",
            "  inflating: by-day/2011-08-16.csv   \n",
            "  inflating: by-day/2011-08-17.csv   \n",
            "  inflating: by-day/2011-08-18.csv   \n",
            "  inflating: by-day/2011-08-19.csv   \n",
            "  inflating: by-day/2011-08-21.csv   \n",
            "  inflating: by-day/2011-08-22.csv   \n",
            "  inflating: by-day/2011-08-23.csv   \n",
            "  inflating: by-day/2011-08-24.csv   \n",
            "  inflating: by-day/2011-08-25.csv   \n",
            "  inflating: by-day/2011-08-26.csv   \n",
            "  inflating: by-day/2011-08-28.csv   \n",
            "  inflating: by-day/2011-08-30.csv   \n",
            "  inflating: by-day/2011-08-31.csv   \n",
            "  inflating: by-day/2011-09-01.csv   \n",
            "  inflating: by-day/2011-09-02.csv   \n",
            "  inflating: by-day/2011-09-04.csv   \n",
            "  inflating: by-day/2011-09-05.csv   \n",
            "  inflating: by-day/2011-09-06.csv   \n",
            "  inflating: by-day/2011-09-07.csv   \n",
            "  inflating: by-day/2011-09-08.csv   \n",
            "  inflating: by-day/2011-09-09.csv   \n",
            "  inflating: by-day/2011-09-11.csv   \n",
            "  inflating: by-day/2011-09-12.csv   \n",
            "  inflating: by-day/2011-09-13.csv   \n",
            "  inflating: by-day/2011-09-14.csv   \n",
            "  inflating: by-day/2011-09-15.csv   \n",
            "  inflating: by-day/2011-09-16.csv   \n",
            "  inflating: by-day/2011-09-18.csv   \n",
            "  inflating: by-day/2011-09-19.csv   \n",
            "  inflating: by-day/2011-09-20.csv   \n",
            "  inflating: by-day/2011-09-21.csv   \n",
            "  inflating: by-day/2011-09-22.csv   \n",
            "  inflating: by-day/2011-09-23.csv   \n",
            "  inflating: by-day/2011-09-25.csv   \n",
            "  inflating: by-day/2011-09-26.csv   \n",
            "  inflating: by-day/2011-09-27.csv   \n",
            "  inflating: by-day/2011-09-28.csv   \n",
            "  inflating: by-day/2011-09-29.csv   \n",
            "  inflating: by-day/2011-09-30.csv   \n",
            "  inflating: by-day/2011-10-02.csv   \n",
            "  inflating: by-day/2011-10-03.csv   \n",
            "  inflating: by-day/2011-10-04.csv   \n",
            "  inflating: by-day/2011-10-05.csv   \n",
            "  inflating: by-day/2011-10-06.csv   \n",
            "  inflating: by-day/2011-10-07.csv   \n",
            "  inflating: by-day/2011-10-09.csv   \n",
            "  inflating: by-day/2011-10-10.csv   \n",
            "  inflating: by-day/2011-10-11.csv   \n",
            "  inflating: by-day/2011-10-12.csv   \n",
            "  inflating: by-day/2011-10-13.csv   \n",
            "  inflating: by-day/2011-10-14.csv   \n",
            "  inflating: by-day/2011-10-16.csv   \n",
            "  inflating: by-day/2011-10-17.csv   \n",
            "  inflating: by-day/2011-10-18.csv   \n",
            "  inflating: by-day/2011-10-19.csv   \n",
            "  inflating: by-day/2011-10-20.csv   \n",
            "  inflating: by-day/2011-10-21.csv   \n",
            "  inflating: by-day/2011-10-23.csv   \n",
            "  inflating: by-day/2011-10-24.csv   \n",
            "  inflating: by-day/2011-10-25.csv   \n",
            "  inflating: by-day/2011-10-26.csv   \n",
            "  inflating: by-day/2011-10-27.csv   \n",
            "  inflating: by-day/2011-10-28.csv   \n",
            "  inflating: by-day/2011-10-30.csv   \n",
            "  inflating: by-day/2011-10-31.csv   \n",
            "  inflating: by-day/2011-11-01.csv   \n",
            "  inflating: by-day/2011-11-02.csv   \n",
            "  inflating: by-day/2011-11-03.csv   \n",
            "  inflating: by-day/2011-11-04.csv   \n",
            "  inflating: by-day/2011-11-06.csv   \n",
            "  inflating: by-day/2011-11-07.csv   \n",
            "  inflating: by-day/2011-11-08.csv   \n",
            "  inflating: by-day/2011-11-09.csv   \n",
            "  inflating: by-day/2011-11-10.csv   \n",
            "  inflating: by-day/2011-11-11.csv   \n",
            "  inflating: by-day/2011-11-13.csv   \n",
            "  inflating: by-day/2011-11-14.csv   \n",
            "  inflating: by-day/2011-11-15.csv   \n",
            "  inflating: by-day/2011-11-16.csv   \n",
            "  inflating: by-day/2011-11-17.csv   \n",
            "  inflating: by-day/2011-11-18.csv   \n",
            "  inflating: by-day/2011-11-20.csv   \n",
            "  inflating: by-day/2011-11-21.csv   \n",
            "  inflating: by-day/2011-11-22.csv   \n",
            "  inflating: by-day/2011-11-23.csv   \n",
            "  inflating: by-day/2011-11-24.csv   \n",
            "  inflating: by-day/2011-11-25.csv   \n",
            "  inflating: by-day/2011-11-27.csv   \n",
            "  inflating: by-day/2011-11-28.csv   \n",
            "  inflating: by-day/2011-11-29.csv   \n",
            "  inflating: by-day/2011-11-30.csv   \n",
            "  inflating: by-day/2011-12-01.csv   \n",
            "  inflating: by-day/2011-12-02.csv   \n",
            "  inflating: by-day/2011-12-04.csv   \n",
            "  inflating: by-day/2011-12-05.csv   \n",
            "  inflating: by-day/2011-12-06.csv   \n",
            "  inflating: by-day/2011-12-07.csv   \n",
            "  inflating: by-day/2011-12-08.csv   \n",
            "  inflating: by-day/2011-12-09.csv   \n"
          ]
        }
      ],
      "source": [
        "!unzip by-day.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EDIKP18FLVPp"
      },
      "outputs": [],
      "source": [
        "spark_df =spark.read.format('csv').option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"by-day/*.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4g7_5l3zhif"
      },
      "source": [
        "Solve the same question using raw SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dUEkb7lmzi6u"
      },
      "outputs": [],
      "source": [
        "#Your answer here\n",
        "spark_df.createOrReplaceTempView(\"ByDay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqeibY3SLVPq"
      },
      "source": [
        "### Using SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "37fNMDLMLVPq"
      },
      "outputs": [],
      "source": [
        "query = spark.sql(\"\"\"\n",
        "  SELECT CustomerID,  sum(UnitPrice * Quantity) AS Total\n",
        "  FROM ByDay\n",
        "  WHERE Description NOT LIKE '%WATER%'\n",
        "  GROUP BY CustomerID\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECRJSWM1LVPq",
        "outputId": "75a7b574-7321-4660-d3df-8e12ab32cb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|             Total|\n",
            "+----------+------------------+\n",
            "|   14452.0|264.44000000000005|\n",
            "|   16916.0| 565.3599999999999|\n",
            "|   17633.0|1242.3400000000001|\n",
            "|   14768.0|             139.5|\n",
            "|   13094.0|1708.8600000000001|\n",
            "|   17884.0|            674.95|\n",
            "|   16596.0|             220.3|\n",
            "|   13607.0| 648.3099999999998|\n",
            "|   14285.0|1589.0100000000002|\n",
            "|   16561.0|            481.87|\n",
            "|   13956.0|1026.4199999999998|\n",
            "|   13533.0|            270.79|\n",
            "|   16629.0| 417.7299999999999|\n",
            "|   17267.0|312.96999999999997|\n",
            "|   13918.0|           1212.84|\n",
            "|   18114.0|            216.35|\n",
            "|   14473.0|234.34000000000003|\n",
            "|   14024.0|             327.7|\n",
            "|   12493.0|            393.39|\n",
            "|   15776.0|241.62000000000003|\n",
            "+----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkZyOfsvLVPr"
      },
      "source": [
        "### Using Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkxrrWD7LVPr",
        "outputId": "16d38c45-d6f0-41c6-8fdb-4e19b024cc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
            "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
            "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
            "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
            "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
            "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
            "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
            "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    22074|6 RIBBONS SHIMMER...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
            "|   580539|    22075|6 RIBBONS ELEGANT...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
            "|   580539|    22076|  6 RIBBONS EMPIRE  |      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
            "|   580539|    22389|PAPERWEIGHT SAVE ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
            "|   580539|    22391|PAPERWEIGHT HOME ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df1 = spark_df.select(spark_df.columns).where(~col(\"Description\").like(\"% WATER %\"))\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ULEVU8neLVPr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "df2 = df1.selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) as total_cost\").groupby(col(\"CustomerID\")).sum(\"total_cost\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JfW1uXL6X5",
        "outputId": "75438aa9-142f-48dd-b00f-56cd2fcc9b29"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|CustomerID|   sum(total_cost)|\n",
            "+----------+------------------+\n",
            "|   14452.0|264.44000000000005|\n",
            "|   16916.0| 565.3599999999999|\n",
            "|   17633.0|1242.3400000000001|\n",
            "|   14768.0|             139.5|\n",
            "|   13094.0|1708.8600000000001|\n",
            "|   17884.0|            674.95|\n",
            "|   16596.0|             220.3|\n",
            "|   13607.0| 648.3099999999998|\n",
            "|   14285.0|1589.0100000000002|\n",
            "|   16561.0|            481.87|\n",
            "|   13956.0|1026.4199999999998|\n",
            "|   13533.0|            270.79|\n",
            "|   16629.0| 417.7299999999999|\n",
            "|   17267.0|312.96999999999997|\n",
            "|   13918.0|           1212.84|\n",
            "|   18114.0|            216.35|\n",
            "|   14473.0|234.34000000000003|\n",
            "|   14024.0|             327.7|\n",
            "|   12493.0|416.78999999999996|\n",
            "|   15776.0|241.62000000000003|\n",
            "+----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej5yOibIilC7"
      },
      "source": [
        "### **Challenge 2 - Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4WKCC_wrYI"
      },
      "source": [
        "We use Pipeline to specify our machine learning workflow. A Pipeline’s stages are specified as an ordered array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS5c1E2x5TdU"
      },
      "source": [
        "Applying the following tasks to create your ML pipeline :\n",
        "\n",
        "1. First, read all the files in the `by-day` folder.\n",
        "2. Next, create a pipeline using the `Pipeline()` function, in which the indexer is applied followed by the assembler.\n",
        "3. Afterwards, fit and transform the data at hand using the pipeline we created which includes: StringIndexer and VectorAssembler. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-UJ0PeFrwaq7"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "sindexer = StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_indexed\")\n",
        "va = VectorAssembler().setInputCols([\"UnitPrice\",\"Quantity\", \"day_of_week_indexed\"]).setOutputCol(\"Unit-Quantity-day_of_week\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cWvOi_ngLVP3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import date_format,col\n",
        "my_df = df1.withColumn(\"day_of_week\",date_format(col(\"InvoiceDate\"),\"EEEE\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J7O3X3CLVP4",
        "outputId": "ddb37a9c-4916-4451-f352-0112f1c2e9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|     Monday|\n",
            "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|     Monday|\n",
            "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22074|6 RIBBONS SHIMMER...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22075|6 RIBBONS ELEGANT...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22076|  6 RIBBONS EMPIRE  |      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22389|PAPERWEIGHT SAVE ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|\n",
            "|   580539|    22391|PAPERWEIGHT HOME ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "my_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Q5_xLXCiLVP4"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "my_pl = Pipeline().setStages([sindexer,va])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zIjCJVB1LVP4"
      },
      "outputs": [],
      "source": [
        "fittedpl = my_pl.fit(my_df)\n",
        "transpl = fittedpl.transform(my_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwGb_e2xLVP5",
        "outputId": "d6da2da8-87af-4ef1-a586-679f55cb62e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-------------------+-------------------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|day_of_week_indexed|Unit-Quantity-day_of_week|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-------------------+-------------------------+\n",
            "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|                2.0|          [1.79,48.0,2.0]|\n",
            "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|                2.0|          [1.25,20.0,2.0]|\n",
            "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|                2.0|          [1.65,24.0,2.0]|\n",
            "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|                2.0|          [1.25,24.0,2.0]|\n",
            "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|                2.0|           [2.55,6.0,2.0]|\n",
            "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|     Monday|                2.0|           [4.95,8.0,2.0]|\n",
            "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|     Monday|                2.0|          [1.69,24.0,2.0]|\n",
            "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|                2.0|           [1.95,8.0,2.0]|\n",
            "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|     Monday|                2.0|           [1.95,8.0,2.0]|\n",
            "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|                2.0|          [1.25,12.0,2.0]|\n",
            "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|     Monday|                2.0|           [1.65,6.0,2.0]|\n",
            "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|     Monday|                2.0|          [0.85,36.0,2.0]|\n",
            "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|     Monday|                2.0|          [1.25,24.0,2.0]|\n",
            "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|                2.0|           [4.25,4.0,2.0]|\n",
            "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|     Monday|                2.0|           [4.25,4.0,2.0]|\n",
            "|   580539|    22074|6 RIBBONS SHIMMER...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|                2.0|          [0.39,24.0,2.0]|\n",
            "|   580539|    22075|6 RIBBONS ELEGANT...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|                2.0|          [0.39,24.0,2.0]|\n",
            "|   580539|    22076|  6 RIBBONS EMPIRE  |      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|                2.0|          [0.39,24.0,2.0]|\n",
            "|   580539|    22389|PAPERWEIGHT SAVE ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|                2.0|          [0.39,12.0,2.0]|\n",
            "|   580539|    22391|PAPERWEIGHT HOME ...|      12|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|     Monday|                2.0|          [0.39,12.0,2.0]|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+-------------------+-------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transpl.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hbx6GG_LLbaW"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab9 - Apache Spark-Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}